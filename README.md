# Proyecto ETL con Airflow, MySQL y Docker üöÄ

Este repositorio contiene un pipeline ETL (Extracci√≥n, Transformaci√≥n y Carga) que utiliza Python para procesar datos desde archivos CSV y una base de datos MySQL. 
Los datos transformados se insertan en un **Data Warehouse (DW)** OLAP denominado `dw_netflix`, dise√±ado para an√°lisis avanzado. Toda la configuraci√≥n del entorno se gestiona mediante contenedores Docker.

---

## üõ†Ô∏è Requisitos

1. **Docker** y **Docker Compose** instalados.
2. **Python 3.8+** (opcional si deseas ejecutar los scripts localmente).

---

## üöÄ Configuraci√≥n e instalaci√≥n

**1. Clona este repositorio:**

git clone <URL_DEL_REPOSITORIO>
cd PROYECTO_ETL

**2. Configura los contenedores Docker: Levanta los servicios definidos en docker-compose.yml:**

docker-compose up -d

**3. Verifica que los contenedores est√©n corriendo:**

docker ps

**4. Carga los datos iniciales: Coloca los archivos CSV en la carpeta data/ seg√∫n el formato esperado.**

üì¶ Uso del ETL

**5. Ejecuta el script principal: El script ETL.py realizar√° las siguientes tareas:**

- Leer los archivos CSV desde la carpeta data/.
- Extraer datos de la base de datos MySQL del contenedor.
- Transformar y limpiar los datos.
- Insertar los datos en el Data Warehouse OLAP dw_netflix.

```
python scripts/ETL.py
```

Verifica los logs: El archivo logs/etl_process.log contiene informaci√≥n detallada del proceso ETL.

üóÉÔ∏è Data Warehouse

El Data Warehouse OLAP est√° configurado en el contenedor MySQL bajo el esquema dw_netflix. Contiene las tablas necesarias para an√°lisis avanzado y generaci√≥n de reportes.
Verifica que las rutas de los archivos CSV sean correctas y que los datos est√©n completos.
